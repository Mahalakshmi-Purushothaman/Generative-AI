{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Langchain"
      ],
      "metadata": {
        "id": "WzgjcDclH1kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community openai huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHxxoUwqIlOI",
        "outputId": "086dc6ce-55b3-4ca0-ece2-4d0e5d835b1c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.34)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Environment"
      ],
      "metadata": {
        "id": "bJ8kAe7sJ_uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"key\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_KEY\"] = \"key\""
      ],
      "metadata": {
        "id": "8k_2_6InIlRY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM"
      ],
      "metadata": {
        "id": "RpOpGpc3LH_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI, HuggingFaceHub\n",
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "A4rfe7_VIlUY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"what would be a good company name for a company that makes colorful socks\"\n",
        "print(llm.predict(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuSpjjtIlXQ",
        "outputId": "07fa1db6-5a9d-4457-e617-ed088c9210eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-89c5f593b0a7>:2: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(llm.predict(text))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Rainbow Sock Co.\"\n",
            "\"Spectrum Socks\"\n",
            "\"Vivid Footwear\"\n",
            "\"Colorful Steps Co.\"\n",
            "\"Pigment Pals\"\n",
            "\"Chroma Socks\"\n",
            "\"Rainbow Threads\"\n",
            "\"Kaleidosock\"\n",
            "\"Chromatic Soles\"\n",
            "\"ColorCraze Socks\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVI8mh4lIlaQ",
        "outputId": "4c43a411-a621-43e7-edfc-7891a645d6d9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Rainbow Threads Co.\" or \"ChromaSocks\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SoLbXXwIldQ",
        "outputId": "51bde880-d21e-4445-df2d-3de12b03a7ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Rainbow Threads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2"
      ],
      "metadata": {
        "id": "DCJa1kPlMdtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI, HuggingFaceHub\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "name = llm.predict(\"I want to open a restaurant for american food. suggest a fancy name for this\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwkOjbwzJ98X",
        "outputId": "fed18c41-6741-4e0b-d803-16592f456231"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Stateside Bistro\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(\"I want to open a restaurant for american food. suggest a fancy name for this\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUQJbw2mJ9_4",
        "outputId": "e6a8d6fd-d2e2-43d0-ab04-411d87238aa4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Stateside Bistro\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face"
      ],
      "metadata": {
        "id": "_ti4g99dNTRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "llm(\"translate English to German: How old are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "XY3d0B1bJ-C_",
        "outputId": "92a45b89-6cd1-4309-e5c0-61b6e1826f40"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Task text2text-generation has no recommended model. Please specify a model explicitly. Visit https://huggingface.co/tasks for more info.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a46632c9cb9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceHub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/flan-t5-large\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"translate English to German: How old are you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: TRY004\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         return (\n\u001b[0;32m-> 1298\u001b[0;31m             self.generate(\n\u001b[0m\u001b[1;32m   1299\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 )\n\u001b[1;32m    962\u001b[0m             ]\n\u001b[0;32m--> 963\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             output = (\n\u001b[0;32m--> 784\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    785\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m             text = (\n\u001b[0;32m-> 1523\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/llms/huggingface_hub.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         response = self.client.post(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mwarning_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m         \u001b[0mprovider_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHFInferenceTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprovider_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprovider_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprovider_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         return self._inner_post(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_providers/hf_inference.py\u001b[0m in \u001b[0;36mmap_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_recommended_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_providers/hf_inference.py\u001b[0m in \u001b[0;36mget_recommended_model\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fetch_recommended_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;34mf\"Task {task} has no recommended model. Please specify a model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\" explicitly. Visit https://huggingface.co/tasks for more info.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Task text2text-generation has no recommended model. Please specify a model explicitly. Visit https://huggingface.co/tasks for more info."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Template"
      ],
      "metadata": {
        "id": "G3ojoCKKQwgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restraunt for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(cuisine=\"Italian\")\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ_Jq-msJ-GQ",
        "outputId": "09a6f7d8-9473-4a6f-88c4-2d3e2d9dcc38"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a restraunt for Italian food. Suggest a fancy name for this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "prompt.format(product=\"colorful socks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iLyKHOZaIlgQ",
        "outputId": "4d9d3981-5e9c-4104-808f-cf0d4880278c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is a good name for a company that makes colorful socks?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain"
      ],
      "metadata": {
        "id": "J5oYpgrxSVhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "llm = OpenAI(temperature=0.9)\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain.run(\"colorful socks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "uhzcSqJ0IljY",
        "outputId": "66cf75b9-b013-4fd9-fade-cb808a6e51dd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-c3855df5cdb6>:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-46-c3855df5cdb6>:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(\"colorful socks\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\"Rainbow Sock Co.\" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.run(\"colorful socks\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKK_SLDlIlmA",
        "outputId": "bf4e9a4c-332e-4563-a86a-e8517068b666"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SpectrumSocks Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents"
      ],
      "metadata": {
        "id": "C-QhmggZTWK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpPDq4EMIlpA",
        "outputId": "f95e6b87-58f6-4931-b3c2-5c6a5694b81e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=fb85cff2fbf5c8498834afa0bef7c959578dc87d88f3f7ecb4a7c0b428ad9e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "1JfE9XwuVbNl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "fyAWylzGVbQ-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools=load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "agent=initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"What was the GDP of USA in 2024?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "QGeJigynVbuO",
        "outputId": "001580a3-de7c-486a-8613-12e5e7b29959"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-52f586fa9106>:3: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent=initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the answer\n",
            "Action: wikipedia\n",
            "Action Input: \"GDP of USA in 2024\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: List of U.S. states and territories by GDP\n",
            "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
            "The data source for the list is the Bureau of Economic Analysis (BEA) in 2024. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"\n",
            "Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.\n",
            "Overall, in the calendar year 2024, the United States' Nominal GDP at Current Prices totaled at $29.017 trillion, as compared to $25.744 trillion in 2022.\n",
            "The three U.S. states with the highest GDPs were California ($4.080 trillion), Texas ($2.695 trillion), and New York ($2.284 trillion). The three U.S. states with the lowest GDPs were Vermont ($45.4 billion), Wyoming ($53.0 billion), and Alaska ($69.8 billion).\n",
            "GDP per capita also varied widely throughout the United States in 2024, with New York ($117,332), Massachusetts ($110,561), and Washington (state) ($108,468) recording the three highest GDP per capita figures in the U.S., while Mississippi ($53,061), Arkansas ($60,276), and West Virginia ($60,783) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita figure far higher than any U.S. state in 2024 at $263,220.\n",
            "\n",
            "\n",
            "\n",
            "Page: Economy of the United States\n",
            "Summary: The United States has a highly developed mixed economy. It is the world's largest economy by nominal GDP and second largest by purchasing power parity (PPP). As of 2024, it has the world's sixth highest nominal GDP per capita and eighth highest GDP per capita by PPP). The U.S. accounted for 26% of the global economy in 2023 in nominal terms, and about 15.5% in PPP terms. The U.S. dollar is the currency of record most used in international transactions and is the world's reserve currency, backed by a large U.S. treasuries market, its role as the reference standard for the petrodollar system, and its linked eurodollar. Several countries use it as their official currency and in others it is the de facto currency. Since the end of World War II, the economy has achieved relatively steady growth, low unemployment and inflation, and rapid advances in technology. \n",
            "The American economy is fueled by high productivity, well developed transportation infrastructure, and extensive natural resources. Americans have the sixth highest average household and employee income among OECD member states. In 2021, they had the highest median household income among OECD countries although the country also had one of the world's highest income inequalities among the developed countries. The largest U.S. trading partners are Canada, Mexico, China, Japan, Germany, South Korea, the United Kingdom, Taiwan, India, and Vietnam. The U.S. is the world's largest importer and second largest exporter. It has free trade agreements with several countries, including Canada and Mexico (through the USMCA), Australia, South Korea, Israel, and several others that are in effect or under negotiation. The U.S. has a highly flexible labor market, where the industry adheres to a hire-and-fire policy, and job security is relatively low. Among OECD nations, the U.S. has a highly efficient and strong social security system; social expenditure stood at roughly 30% of GDP.  \n",
            "The United States is the world's largest producer of petroleum and natural gas. In 2016, it was the world's largest trading country and second largest manufacturer, with American manufacturing making up a fifth of the global total. The U.S. not only has the largest internal market for\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The GDP of USA in 2024 was $29.017 trillion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GDP of USA in 2024 was $29.017 trillion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory"
      ],
      "metadata": {
        "id": "8CCEKnG6V6XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restraunt for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name = chain.run(\"Italian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL7-Ik4_VbxF",
        "outputId": "c92a6594-739b-4ff7-dd67-95b4c94e4ca0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"La Bella Cucina\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.memory"
      ],
      "metadata": {
        "id": "-mdJgv2gVbzd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(chain.memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auPXrkwzVb9m",
        "outputId": "4178e955-6001-448f-a4f9-64eb618c32d8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationBufferMemory"
      ],
      "metadata": {
        "id": "uktxH0BHWzSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run(\"Italian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF-kXmTRVcAv",
        "outputId": "02f5263c-5bfb-4bf8-b990-479241151a51"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-69495205ee09>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"La Bella Cucina\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name=chain.run(\"Indian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uksYgr2AVcE1",
        "outputId": "b788d50b-5c23-415b-a750-2be4d441e12e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name=chain.run(\"Arabian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSsYgfiGVcI2",
        "outputId": "5c463bf8-d89b-4706-dcb0-06cf6c57cb56"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Aladdin's Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name=chain.run(\"Mexican\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5O8IYiBXM1Y",
        "outputId": "5b3d63b1-1105-46e7-b373-da4cce4b3c73"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Sabroso Cantina\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name=chain.run(\"American\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSsWQPfMXM4v",
        "outputId": "c52b2709-bed3-4339-d86e-2f242e4fe44a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Stateside Eats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEcos_D4XM7I",
        "outputId": "774f11d3-eb15-4a10-de57-960497c02a82"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Italian\n",
            "AI: \n",
            "\n",
            "\"La Bella Cucina\"\n",
            "Human: Indian\n",
            "AI: \n",
            "\n",
            "\"Spice Palace\"\n",
            "Human: Arabian\n",
            "AI: \n",
            "\n",
            "\"Aladdin's Palace\"\n",
            "Human: Mexican\n",
            "AI: \n",
            "\n",
            "\"El Sabroso Cantina\"\n",
            "Human: American\n",
            "AI: \n",
            "\n",
            "\"Stateside Eats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationChain"
      ],
      "metadata": {
        "id": "VIbRwCV8XgIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6fW44wtXcig",
        "outputId": "e67832c7-7c29-45dc-9fca-6997e49d6b8d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"who won the first cricket world cup?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "sdxANBppXM9v",
        "outputId": "6f109524-f355-475f-be4a-f41a399b30af"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The first cricket world cup was won by the West Indies in 1975. The final match was held at Lord's Cricket Ground in London, England, and the West Indies defeated Australia by 17 runs. The West Indies team was led by captain Clive Lloyd and included players such as Viv Richards, Gordon Greenidge, and Michael Holding. The tournament was organized by the International Cricket Council (ICC) and featured eight teams, including England, Australia, and India.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"who was the captain of the winning team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V0kGICriXNAP",
        "outputId": "9f152bb2-386a-4881-c20a-a50b84a9c896"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The captain of the winning team was Clive Lloyd.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNsdrWNsXNCv",
        "outputId": "863cab52-3327-4d40-b512-45b4c4750345"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: who won the first cricket world cup?\n",
            "AI:  The first cricket world cup was won by the West Indies in 1975. The final match was held at Lord's Cricket Ground in London, England, and the West Indies defeated Australia by 17 runs. The West Indies team was led by captain Clive Lloyd and included players such as Viv Richards, Gordon Greenidge, and Michael Holding. The tournament was organized by the International Cricket Council (ICC) and featured eight teams, including England, Australia, and India.\n",
            "Human: who was the captain of the winning team?\n",
            "AI:  The captain of the winning team was Clive Lloyd.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "8-p4lMqjYtSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7), memory=memory)\n",
        "convo.run(\"who won the first cricket world cup?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "7ohTFxXyXNFP",
        "outputId": "98786089-b2e6-4930-ac8a-7e0bbefa94ae"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-e2cbaecc9771>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The first Cricket World Cup was held in 1975 and was won by the West Indies. The tournament was hosted by England and featured eight teams, with the West Indies defeating Australia in the final by 17 runs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"how much is 5+5?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "J6KEmhRXXNIA",
        "outputId": "45bdfc5a-8d16-4394-d848-82ef00f06798"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  5+5 is equal to 10.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"who was the captain of the winning team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HJG34WwsZEa6",
        "outputId": "a2755e2f-fc10-4a11-8775-bfe5a2503c32"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry, I do not have enough context to accurately answer that question. Can you provide me with more information?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjIM1HpXZEd6",
        "outputId": "93f78db6-4b77-4b65-829e-b82b47ac70de"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: who was the captain of the winning team?\n",
            "AI:  I'm sorry, I do not have enough context to accurately answer that question. Can you provide me with more information?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_X86jUobZEmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaKdzc51ZEo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}