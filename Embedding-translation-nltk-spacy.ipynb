{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496e4676-9995-49a3-870a-d95a87b3f45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mahav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c6889-b775-4e6f-83ef-81f3079ef02c",
   "metadata": {},
   "source": [
    "Embedding in Generative AI\n",
    "In Generative AI, embedding refers to the process of transforming high-dimensional data (such as text, images, or audio) into dense, lower-dimensional vector representations. These embeddings preserve the semantic properties of the input data, making it easier for models to process and understand.\n",
    "\n",
    "Embeddings are used in various machine learning tasks, such as Natural Language Processing (NLP), recommendation systems, and computer vision, where they help capture the underlying structure or meaning of the data in a format suitable for further analysis or generation.\n",
    "\n",
    "Types of Embedding in Generative AI\n",
    "Word Embeddings\n",
    "Word embeddings are used to represent words or tokens in a continuous vector space, where semantically similar words are represented by vectors that are close to each other in the space.\n",
    "\n",
    "Example Algorithms: Word2Vec, GloVe, FastText\n",
    "\n",
    "Banking Scenario:\n",
    "Word embeddings can be used to analyze customer queries in a banking chatbot. By embedding terms like “credit,” “loan,” and “account” into vectors, the chatbot can match user intent more accurately, even when words are phrased differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7835aece-b857-4a14-87a1-b7856b0a2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
      "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
      " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
      "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
      "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
      "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
      "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
      " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
      " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
      " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
      " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
      " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
      "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
      " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
      "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
      "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"bank\", \"loan\", \"offer\"], [\"credit\", \"card\", \"balance\"], [\"withdraw\", \"account\", \"amount\"]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "vector = model.wv['bank']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40276dd3-4f85-45b6-9465-e227900f810e",
   "metadata": {},
   "source": [
    "Sentence Embeddings\n",
    "Sentence embeddings represent entire sentences or phrases as vectors. They capture the semantic meaning of a sentence or a document and can be used to compare and match documents or classify text.\n",
    "\n",
    "Example Algorithms: Universal Sentence Encoder, BERT, RoBERTa\n",
    "\n",
    "Banking Scenario:\n",
    "Sentence embeddings can be used in customer support systems to identify customer queries regarding loans, balance inquiries, or account status, allowing the system to route the query to the correct department or provide an appropriate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1e439-e482-436e-b51e-579e515b9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "sentences = [\"The customer wants to apply for a loan.\", \"A person is asking about loan options.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793086d-9d97-4225-bc46-da478386b30a",
   "metadata": {},
   "source": [
    "Document Embeddings\n",
    "Document embeddings are similar to sentence embeddings but operate at a higher level. They are used to represent entire documents or lengthy texts.\n",
    "\n",
    "Example Algorithms: Doc2Vec, BERT for document-level embeddings\n",
    "\n",
    "Banking Scenario:\n",
    "Document embeddings can be useful for organizing loan agreement documents or customer complaint texts. Banks can use document embeddings to categorize or automatically extract relevant information from these documents, improving the efficiency of customer support and legal teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8ef96c-497b-47cd-aa9f-be5ff5a45161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0119142  -0.01890612 -0.00945078 -0.01908035 -0.0233835  -0.00884717\n",
      "  0.01290698 -0.01533926  0.00040241 -0.02285166  0.00069051 -0.00709472\n",
      " -0.01313953  0.0030447  -0.00140546  0.01790848 -0.01800241 -0.00844084\n",
      "  0.02096478 -0.00268845]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(words=[\"The\", \"loan\", \"terms\", \"are\", \"confusing\"], tags=[\"loan_terms\"])]\n",
    "model = Doc2Vec(documents, vector_size=20, window=2, min_count=1, workers=4)\n",
    "vector = model.infer_vector([\"The\", \"loan\", \"terms\", \"are\", \"confusing\"])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a271e-1289-42af-bd7c-107a964de5e6",
   "metadata": {},
   "source": [
    "Image Embeddings\n",
    "Image embeddings are used to represent images in a lower-dimensional space. These embeddings can be used for tasks like image classification, retrieval, or generative image modeling.\n",
    "\n",
    "Example Algorithms: Convolutional Neural Networks (CNNs), Vision Transformers (ViTs)\n",
    "\n",
    "Banking Scenario:\n",
    "Image embeddings could be used for analyzing documents in the form of scanned bank forms or loan applications. For instance, image embeddings can identify if a document contains a signature or extract information from a check image for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465e8904-dd4e-4181-a850-cdd3a43bcc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 0us/step\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bank_logo.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbank_logo.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# path to an image\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mload_img(img_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m      9\u001b[0m img_array \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[0;32m     10\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank_logo.jpg'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "img_path = 'bank_logo.jpg'  # path to an image\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "embedding = model.predict(img_array)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc4077-1988-4130-b77e-b557a429134d",
   "metadata": {},
   "source": [
    "Graph Embeddings\n",
    "Graph embeddings are used for data that is structured in the form of graphs, such as social networks or transaction systems. Nodes and edges are mapped into a vector space while preserving the graph structure.\n",
    "\n",
    "Example Algorithms: Node2Vec, GraphSAGE\n",
    "\n",
    "Banking Scenario:\n",
    "In banking, graph embeddings can be used to model financial transactions, detect fraud, or analyze customer relationships. For example, graph embeddings could help identify clusters of customers with similar spending behavior or detect unusual transaction patterns indicative of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1661ec-aa7d-4a25-bace-b3e8be35e49b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'node2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnode2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      4\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mkarate_club_graph()  \u001b[38;5;66;03m# A sample social network graph\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'node2vec'"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.karate_club_graph()  # A sample social network graph\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit()\n",
    "embedding = model.wv['0']  # Vector representation of node 0\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f05d4b-14f3-45f7-852c-cf4e19b3dfa4",
   "metadata": {},
   "source": [
    "Multimodal Embeddings\n",
    "Multimodal embeddings are a combination of different types of embeddings, such as text and image embeddings, that are fused into a unified representation.\n",
    "\n",
    "Example Algorithms: CLIP (Contrastive Language-Image Pretraining), T5, ViLBERT\n",
    "\n",
    "Banking Scenario:\n",
    "Multimodal embeddings can help analyze both images and text together, which is useful in scenarios like detecting fraudulent checks or verifying documents. For instance, a customer might submit a check image and a transaction description, and the bank system can cross-reference both modalities to verify the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1449ba-c9f8-4afb-aa1b-fa3def2f2d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfd476769f84f4ca7c58184cd6c2f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahav\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mahav\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8aa9d918fe4c79a938fec40182a177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e448781ed47f19085dc2d4fdd2161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d90e1eda0f4de9b242b03d009ca459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e7c065d20443ad9255137876849ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b7f167f58b4003a6daa00ad3874ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb0f293da64ace8f191514b024ab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd1948400e64f6fafd713dbf15200a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CLIPModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA bank logo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbank_logo.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Path to image file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtext, images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "text = [\"A bank logo\"]\n",
    "image = Image.open(\"bank_logo.jpg\")  # Path to image file\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "image_embedding = outputs.image_embeds\n",
    "text_embedding = outputs.text_embeds\n",
    "\n",
    "print(image_embedding)\n",
    "print(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5271208-3b12-4e71-a0c6-e8c1ba1f7e40",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Each type of embedding provides a different approach to encoding complex data structures into vector spaces, and their application varies based on the data type and the task at hand. In the banking sector, these embeddings are crucial for automating processes, analyzing customer interactions, improving security measures, and enhancing customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c6dd4-4d3a-4d59-b2f7-9fa89118481f",
   "metadata": {},
   "source": [
    "Text Cleaning in Natural Language Processing (NLP)\n",
    "Text cleaning is a crucial preprocessing step in Natural Language Processing (NLP) that involves removing or transforming raw text data into a cleaner and more usable format for analysis and modeling. Clean text ensures that the NLP models can focus on relevant features, leading to better performance and more accurate results.\n",
    "\n",
    "Key Terminologies in Text Cleaning\n",
    "Tokenization Tokenization refers to splitting text into smaller units called tokens. These tokens can be words, subwords, or characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b470a0fc-37f5-4ddb-86a0-69946ba319a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loan application has been approved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"The loan application has been approved.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce11953-64db-450b-81d4-0f88f17dd78f",
   "metadata": {},
   "source": [
    "Lowercasing Lowercasing involves converting all text into lowercase to maintain consistency and avoid treating the same words with different cases as distinct entities (e.g., \"Loan\" and \"loan\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c571dec1-63f6-4548-9c5c-016be0226b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loan application has been approved.\n"
     ]
    }
   ],
   "source": [
    "text = \"The Loan application has been Approved.\"\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ca12c-1bfb-44de-9362-678405bcf8c6",
   "metadata": {},
   "source": [
    "Removing Punctuation Punctuation marks such as commas, periods, exclamation marks, etc., often do not contribute to text meaning in many NLP tasks and are removed during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45065acd-f1f3-4e3b-8b96-927c7ef9adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loan application approved\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "text = \"The loan application, approved!.\"\n",
    "text = ''.join([char for char in text if char not in string.punctuation])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97010b86-7827-473c-a785-54b4221cbad6",
   "metadata": {},
   "source": [
    "Stopwords Removal Stopwords are common words (e.g., \"the\", \"is\", \"and\") that generally do not carry meaningful content and are removed in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f0ce4f-f390-4548-8467-471bb56e844e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 2\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loan application has been approved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"The loan application has been approved.\"\n",
    "tokens = word_tokenize(text)\n",
    "cleaned_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9e8b2-47ee-4d4b-9ac8-593e4bbd21b1",
   "metadata": {},
   "source": [
    "Stemming Stemming reduces words to their base or root form. It removes suffixes and converts words like \"running\" to \"run\" or \"better\" to \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a96fd7-1ea7-417b-a76c-217cc09cfd87",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ps \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning better\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [ps\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text)]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed_words)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "text = \"running better\"\n",
    "stemmed_words = [ps.stem(word) for word in word_tokenize(text)]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d55fb-c2a8-4ec4-9304-7384f00ecc19",
   "metadata": {},
   "source": [
    "Lemmatization Lemmatization is similar to stemming but more advanced, as it involves reducing a word to its dictionary form (lemma). It considers the context of the word (e.g., \"better\" becomes \"good\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f83685f-cc4e-47ed-a9ad-3799f180805f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning better\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text)]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_words)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"running better\"\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14879e3-a091-43d5-85cc-eeb01e64cd1f",
   "metadata": {},
   "source": [
    "Removing Numbers Numbers may not always carry meaningful information and are often removed during cleaning, especially in textual data like news articles or social media comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04bbbd9-49e2-40f1-9fe0-d10a3c2361b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loan amount is  dollars.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"The loan amount is 2500 dollars.\"\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3caa6-32c4-4867-9d55-8448ab3789d7",
   "metadata": {},
   "source": [
    "Removing Special Characters Special characters, such as non-alphanumeric symbols, might not be necessary for text analysis and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce2a30a4-b582-48df-a283-fd618aee8c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan amount 2500\n"
     ]
    }
   ],
   "source": [
    "text = \"Loan amount: $2500!!!\"\n",
    "text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bd544-4aa9-4051-9861-3e3bf088c0fc",
   "metadata": {},
   "source": [
    "Whitespace Removal Extra whitespaces (leading, trailing, or multiple consecutive spaces) are cleaned to ensure uniformity in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7db714-e9ec-4516-a683-5b0b6f629d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loan application has been approved.\n"
     ]
    }
   ],
   "source": [
    "text = \"   The  loan  application  has been   approved.   \"\n",
    "text = ' '.join(text.split())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eedc35-a841-4dae-a249-c14e9116a351",
   "metadata": {},
   "source": [
    "Spelling Correction Correcting spelling errors to improve the quality of the text for better modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9be209c-c3d2-4287-bc6d-f4d1d2a95ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[0;32m      2\u001b[0m spell \u001b[38;5;241m=\u001b[39m SpellChecker()\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loand application has been approved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "text = \"The loand application has been approved.\"\n",
    "corrected_text = ' '.join([spell.correction(word) for word in text.split()])\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768144dd-9e6c-4b07-af28-864c2ce9ef8d",
   "metadata": {},
   "source": [
    "Domain-Specific Scenarios and Python Scripts\n",
    "1. Banking Domain\n",
    "In banking, text cleaning can be used to process customer feedback, chat logs, or loan applications. For example, if a bank receives user queries like “I need a loan of 10000 usd to buy a house,” text cleaning ensures that the model understands the key details without distraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a68cf83-f710-4c26-8f35-69d21ec4693c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     23\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample text from a customer query\n",
    "text = \"I need a Loan of 10000 USD to buy a house!!! Please help.\"\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Stem the words\n",
    "ps = PorterStemmer()\n",
    "cleaned_tokens = [ps.stem(word) for word in tokens]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6be2b-e5bc-4027-83e2-b87413623903",
   "metadata": {},
   "source": [
    "2. Insurance Domain\n",
    "In insurance, text cleaning can be applied to customer queries about policies, claims, or premiums. A common task could be cleaning claim descriptions or feedback like \"The premium for my car insurance is too high!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3b5edf-cffc-4287-9c45-e1fae741bd7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     23\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample text from an insurance customer query\n",
    "text = \"The Premium for my Car Insurance is too high!!!\"\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4543e53-2313-473c-a7a2-1292126eee1a",
   "metadata": {},
   "source": [
    "3. Healthcare Domain\n",
    "In healthcare, text cleaning is essential when processing patient reviews, medical reports, or electronic health records (EHRs). Cleaning medical terms and removing irrelevant noise helps in tasks like medical text classification or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffc3d5d7-3a11-4a02-b50c-c10fdbaf8b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([char \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m char\u001b[38;5;241m.\u001b[39misdigit()])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     20\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mahav/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mahav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample text from a healthcare report\n",
    "text = \"Patient has a fever of 103.5°F. Need immediate treatment!\"\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation and numbers\n",
    "text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Stem the words\n",
    "ps = PorterStemmer()\n",
    "cleaned_tokens = [ps.stem(word) for word in tokens]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603e545-735b-452d-948f-e9ee65914d40",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Text cleaning is an essential step in NLP, and different techniques (like tokenization, stopword removal, stemming, lemmatization, etc.) help improve the quality of text data. Whether in banking, insurance, or healthcare, text cleaning ensures that models process data efficiently, making it ready for further analysis or predictive modeling. The above Python scripts demonstrate how text cleaning can be applied to domain-specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c5290-0964-4cb8-9cb6-1c63d070cc42",
   "metadata": {},
   "source": [
    "Vectorization in Natural Language Processing (NLP)\n",
    "Vectorization in Natural Language Processing (NLP) is the process of converting text data into numerical form so that it can be processed by machine learning models. NLP algorithms typically cannot process raw text data directly; therefore, vectorization maps words, sentences, or documents into vectors (arrays of numbers) that represent the semantic meaning of the text.\n",
    "\n",
    "Vectorization techniques vary in complexity and accuracy. The most basic methods like Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) are simple, while more sophisticated methods like Word2Vec, GloVe, and BERT embeddings capture deeper semantic meaning.\n",
    "\n",
    "Key Terminologies in Vectorization\n",
    "Bag-of-Words (BoW)\n",
    "The Bag-of-Words model is one of the simplest text vectorization techniques. It represents text as an unordered set of words (tokens) and counts how often each word appears in the document. However, BoW ignores the order and semantics of the words.\n",
    "\n",
    "Example:\n",
    "Text: \"The loan is approved.\"\n",
    "Bag-of-Words representation: {\"The\": 1, \"loan\": 1, \"is\": 1, \"approved\": 1}\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "TF-IDF is a statistical measure that evaluates the importance of a word within a document relative to its frequency in the corpus. It reduces the importance of words that appear very frequently across documents (e.g., \"the\", \"is\").\n",
    "\n",
    "Formula:   TF - IDF ( t , d ) = TF ( t , d ) × IDF ( t )\n",
    "Where: TF(t, d) is the term frequency of term & IDF(t) is the inverse document frequency of term t across all documents\n",
    "\n",
    "Word2Vec (Word Embeddings)\n",
    "Word2Vec is a neural network-based model for generating word embeddings, where each word is represented by a dense vector. These vectors capture semantic relationships between words, so similar words have vectors that are close together in the vector space.\n",
    "\n",
    "Example: Words like \"king\" and \"queen\" will have embeddings that are close in the vector space, as they are semantically related.\n",
    "GloVe (Global Vectors for Word Representation)\n",
    "GloVe is another word embedding technique that captures semantic relationships by aggregating global word-word co-occurrence statistics from a corpus.\n",
    "\n",
    "Transformers (e.g., BERT, GPT)\n",
    "Transformers are advanced models that generate contextual embeddings, meaning the same word can have different embeddings depending on the context in which it appears. Models like BERT and GPT are widely used for creating contextual embeddings.\n",
    "Banking Scenario:\n",
    "For a banking chatbot, you might use BoW to transform customer queries into numerical vectors and compare them against predefined intents, such as \"loan application\" or \"account balance inquiry\".\n",
    "\n",
    "Vectorization Methods: 1. Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ec7a8-3017-43ea-a85e-e170f64eb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data (e.g., customer reviews)\n",
    "documents = [\n",
    "    \"I want to apply for a loan\",\n",
    "    \"My loan application was approved\",\n",
    "    \"I need help with insurance\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents to obtain BoW representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense array and print\n",
    "print(X.toarray())\n",
    "\n",
    "# Get the feature names (words)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97330260-712e-4230-8dd1-1eb64ece4ba8",
   "metadata": {},
   "source": [
    "2. TF-IDF Vectorization: Insurance Scenario:\n",
    "In the insurance industry, TF-IDF could be used to vectorize customer feedback about insurance policies or claims. Higher TF-IDF scores for words like \"policy\", \"claim\", or \"premium\" would indicate that these words are particularly important to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5ca86-f28f-452c-8467-d32ea26cd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"I want to apply for a loan\",\n",
    "    \"My loan application was approved\",\n",
    "    \"I need help with insurance\"\n",
    "]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents to obtain TF-IDF representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense array and print\n",
    "print(X.toarray())\n",
    "\n",
    "# Get the feature names (words)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8490bc7-1e4c-44ba-a7a9-10f9fae5a87b",
   "metadata": {},
   "source": [
    "3. Word2Vec (Word Embeddings): Healthcare Scenario:\n",
    "In the healthcare domain, Word2Vec could be used to vectorize medical terms, such as \"treatment\", \"hospital\", \"patient\", and identify semantic relationships between them. For example, \"hospital\" and \"clinic\" may have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75b37e-d3da-43c6-8e49-f709a6c7ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample text data (customer queries)\n",
    "sentences = [\n",
    "    [\"apply\", \"loan\", \"customer\", \"service\"],\n",
    "    [\"approved\", \"loan\", \"bank\", \"status\"],\n",
    "    [\"insurance\", \"claim\", \"process\", \"help\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get the vector representation of the word \"loan\"\n",
    "vector = model.wv[\"loan\"]\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc3a0e-c90d-43b6-9fe4-330456bad42e",
   "metadata": {},
   "source": [
    "4. GloVe Embeddings\n",
    "Banking Scenario:\n",
    "Banks can use GloVe to find semantic relationships between loan-related terms such as \"mortgage\", \"credit\", and \"debt\". This can help in clustering customer inquiries or classifying loan types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e85ffb-3e1e-4bca-bbe5-557b96b7a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe vectors (assuming GloVe vectors are in 'glove.6B.50d.txt')\n",
    "def load_glove_vectors(file_path):\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "# Example of using pre-trained GloVe vectors\n",
    "glove_model = load_glove_vectors('glove.6B.50d.txt')\n",
    "word_vector = glove_model.get('loan', None)\n",
    "print(word_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b41ce-0c2b-4216-9bd6-6ee4b06038e9",
   "metadata": {},
   "source": [
    "5. Transformers (e.g., BERT): Insurance Scenario:\n",
    "BERT can be used to understand complex insurance documents or customer feedback. By embedding a query like \"What is the claim process for health insurance?\" into BERT, you can retrieve context-aware embeddings to accurately match it to relevant policy information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720e75a-2cea-4f41-be9f-91c545e341d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"I want to apply for a loan\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the embeddings for the [CLS] token (first token of the sentence)\n",
    "embedding = outputs.last_hidden_state[0][0]\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39193c7a-c3d3-4dc5-a0f9-d5ae9adf147e",
   "metadata": {},
   "source": [
    "Domain-Specific Scenarios: Banking\n",
    "Vectorization in banking could be used for loan application processing, fraud detection, customer queries, or email classification. BoW or TF-IDF could be used to analyze customer service inquiries and classify them into categories like \"loan status\" or \"account balance inquiry\". Loan Application Processing and Customer Queries Classification: In this scenario, we will use TF-IDF and BoW to analyze customer queries about loan status or account balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24468a14-4991-4d3d-a671-1abe38102f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data (Customer Queries and Categories)\n",
    "data = {\n",
    "    \"query\": [\n",
    "        \"What is the status of my loan application?\",\n",
    "        \"How do I apply for a loan?\",\n",
    "        \"I need to check my account balance\",\n",
    "        \"Can I withdraw money from my account?\",\n",
    "        \"When will my loan get approved?\",\n",
    "        \"I forgot my account password\"\n",
    "    ],\n",
    "    \"category\": [\n",
    "        \"Loan Status\",\n",
    "        \"Loan Application\",\n",
    "        \"Account Balance\",\n",
    "        \"Account Inquiry\",\n",
    "        \"Loan Status\",\n",
    "        \"Account Recovery\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(df['query'])  # Vectorize the queries\n",
    "y = df['category']  # Labels (Categories)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (Naive Bayes)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Testing with a new query\n",
    "new_query = [\"How do I apply for a loan?\"]\n",
    "new_vector = vectorizer.transform(new_query)\n",
    "prediction = clf.predict(new_vector)\n",
    "print(f\"Predicted Category: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6caa1c-e583-4883-b46e-9a4cab6351bb",
   "metadata": {},
   "source": [
    "2. Insurance: For insurance, vectorization techniques can be used to analyze claims, customer feedback, and policy descriptions. For instance, customer queries about the status of an insurance claim could be vectorized using TF-IDF or Word2Vec to match the query with the right answer or support team. Analyzing Claims and Customer Feedback: In this insurance scenario, we will use TF-IDF to vectorize customer feedback and classify them into categories like \"Claim Status\" or \"Policy Information\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3532d6b-3364-408d-9845-d40c69c70351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data (Customer Queries about insurance)\n",
    "data = {\n",
    "    \"query\": [\n",
    "        \"What is the status of my claim?\",\n",
    "        \"How can I file a new claim?\",\n",
    "        \"I need information about my health policy\",\n",
    "        \"What is the claim process for auto insurance?\",\n",
    "        \"Can I change the beneficiary of my life insurance policy?\",\n",
    "        \"I want to know the premium for my car insurance\"\n",
    "    ],\n",
    "    \"category\": [\n",
    "        \"Claim Status\",\n",
    "        \"Claim Filing\",\n",
    "        \"Policy Information\",\n",
    "        \"Claim Process\",\n",
    "        \"Policy Change\",\n",
    "        \"Premium Inquiry\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(df['query'])  # Vectorize the queries\n",
    "y = df['category']  # Labels (Categories)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (Naive Bayes)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Testing with a new query\n",
    "new_query = [\"What is the claim process for auto insurance?\"]\n",
    "new_vector = vectorizer.transform(new_query)\n",
    "prediction = clf.predict(new_vector)\n",
    "print(f\"Predicted Category: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9d349-15f8-4655-ac76-aec8ace932c8",
   "metadata": {},
   "source": [
    "3. Healthcare: In healthcare, vectorization could be applied to patient medical records, clinical notes, or doctor-patient conversations. BERT and Word2Vec would help in extracting meaningful insights, such as identifying the relationship between symptoms, diagnosis, and treatment recommendations. Extracting Meaningful Insights from Medical Records Using Word2Vec: In this healthcare scenario, we will use Word2Vec to analyze medical terms, extract relationships between them, and classify clinical notes or doctor-patient interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc5ff2-d13b-4e0b-96ab-2f7dc917a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Sample medical sentences (clinical notes)\n",
    "sentences = [\n",
    "    [\"patient\", \"diagnosed\", \"with\", \"diabetes\", \"type\", \"2\"],\n",
    "    [\"doctor\", \"prescribed\", \"insulin\", \"for\", \"diabetes\"],\n",
    "    [\"patient\", \"complains\", \"of\", \"headache\", \"and\", \"fever\"],\n",
    "    [\"doctor\", \"recommends\", \"a\", \"CT\", \"scan\", \"for\", \"further\", \"evaluation\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Example: Extracting the vector for a word (e.g., \"diabetes\")\n",
    "word_vector = model.wv[\"diabetes\"]\n",
    "print(\"Vector for 'diabetes':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example: Finding the most similar words to \"diabetes\"\n",
    "similar_words = model.wv.most_similar(\"diabetes\", topn=3)\n",
    "print(\"Most similar words to 'diabetes':\")\n",
    "print(similar_words)\n",
    "\n",
    "# Example: Averaging word vectors for a sentence (e.g., a clinical note)\n",
    "sentence = [\"patient\", \"diagnosed\", \"with\", \"diabetes\", \"type\", \"2\"]\n",
    "sentence_vector = np.mean([model.wv[word] for word in sentence], axis=0)\n",
    "print(\"Vector for the sentence:\")\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee810c9-58d2-4835-9f89-b0fa5c7f23d3",
   "metadata": {},
   "source": [
    "Conclusion: Vectorization plays a critical role in transforming raw text data into numerical features that machine learning models can work with. Whether it's Bag-of-Words, TF-IDF, or advanced methods like Word2Vec, GloVe, and BERT, each technique has its strengths and application scenarios in domains like banking, insurance, and healthcare. By converting text into meaningful vectors, these techniques allow for better analysis, classification, and prediction in various industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6ea71-1341-43db-9309-f73d5e14124d",
   "metadata": {},
   "source": [
    "Translation in Natural Language Processing (NLP)\n",
    "Translation in NLP refers to the process of converting text from one language (source language) to another language (target language). The goal is to preserve the semantic meaning of the original text while expressing it in the target language. NLP translation models use a variety of techniques ranging from rule-based methods, statistical models, and to the most advanced machine learning-based approaches, such as Neural Machine Translation (NMT).\n",
    "\n",
    "Key Terminologies in Translation\n",
    "Source Language\n",
    "The language from which text is being translated. For example, if translating from English to Spanish, English is the source language.\n",
    "\n",
    "Target Language\n",
    "The language into which the text is translated. Continuing the previous example, Spanish is the target language.\n",
    "\n",
    "Machine Translation (MT)\n",
    "The automatic process of translating text from one language to another using software and algorithms.\n",
    "\n",
    "Neural Machine Translation (NMT)\n",
    "A state-of-the-art technique that uses deep learning models, particularly neural networks, to improve the quality and fluency of translations. NMT learns from large datasets of translated texts to improve translations over time.\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "A model architecture often used in Neural Machine Translation. The encoder reads and encodes the input text, and the decoder produces the translated output text.\n",
    "\n",
    "Attention Mechanism\n",
    "A technique in NMT where the model learns which parts of the input sentence are most relevant when translating a given word, improving translation accuracy, especially for long sentences.\n",
    "\n",
    "Pre-trained Models\n",
    "Pre-trained translation models like OpenAI’s GPT, Google's BERT, or models from Hugging Face (e.g., MarianMT, mBART) that have been trained on large multilingual datasets. These models can be fine-tuned or used directly for translation tasks.\n",
    "\n",
    "Machine Translation Approaches\n",
    "Rule-Based Machine Translation (RBMT)\n",
    "A traditional approach that relies on linguistic rules and dictionaries to translate text. It is usually limited by the comprehensiveness of the rules and the dictionaries.\n",
    "\n",
    "Statistical Machine Translation (SMT)\n",
    "This approach uses statistical models based on probabilities derived from bilingual text corpora. It includes techniques like phrase-based translation.\n",
    "\n",
    "Neural Machine Translation (NMT)\n",
    "A more recent and advanced approach that uses deep learning models, such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers, to learn translations directly from data.\n",
    "\n",
    "Translation in Banking, Insurance, and Healthcare\n",
    "Banking: Translating customer queries, account information, loan terms, etc., to cater to a multi-lingual customer base.\n",
    "Insurance: Translating insurance policies, claims, and customer inquiries to assist in communication with clients in different languages.\n",
    "Healthcare: Translating medical records, patient inquiries, doctor’s notes, and health-related information to ensure clear communication across language barriers.\n",
    "Python Scripts for Translation in Different Domains\n",
    "Below are Python scripts demonstrating translation in the Banking, Insurance, and Healthcare domains using pre-trained models available via Hugging Face Transformers library.\n",
    "\n",
    "1. Banking: Customer Queries Translation: In a banking scenario, you may want to translate customer service queries from multiple languages to a single language (e.g., English) for processing.\n",
    "\n",
    "Python Script using Hugging Face’s MarianMT (English to Spanish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031ae5c-73c6-4ace-8713-9e5f1b7ccf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'  # English to Spanish\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example banking customer query (in English)\n",
    "query_in_english = \"What is the status of my loan application?\"\n",
    "\n",
    "# Tokenize and translate the query\n",
    "tokens = tokenizer(query_in_english, return_tensors=\"pt\", padding=True)\n",
    "translated_tokens = model.generate(**tokens)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original (English): {query_in_english}\")\n",
    "print(f\"Translated (Spanish): {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3705a-ee85-4a98-a742-613b3c5efd61",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The customer query \"What is the status of my loan application?\" is translated from English to Spanish using the MarianMT model, which is a multilingual translation model available on Hugging Face.\n",
    "This can be used in a multilingual banking system to automatically translate customer queries for further analysis.\n",
    "2. Insurance: Translating Customer Feedback and Claims\n",
    "In the insurance domain, translating customer feedback and claims from multiple languages can improve customer support and claim processing.\n",
    "\n",
    "Python Script using Hugging Face’s MarianMT (German to English):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b977d-a1d1-4d75-931f-393296455445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-de-en'  # German to English\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example insurance-related text in German (customer claim)\n",
    "claim_in_german = \"Ich möchte den Status meines Versicherungsanspruchs wissen.\"\n",
    "\n",
    "# Tokenize and translate the claim\n",
    "tokens = tokenizer(claim_in_german, return_tensors=\"pt\", padding=True)\n",
    "translated_tokens = model.generate(**tokens)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original (German): {claim_in_german}\")\n",
    "print(f\"Translated (English): {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746f361-47f8-4544-b44e-db91fa75fe68",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "This script demonstrates how to translate an insurance claim from German to English.\n",
    "It can be applied in situations where insurance companies have customers from different linguistic backgrounds, and they need to process claims in multiple languages.\n",
    "3. Healthcare: Translating Medical Notes and Patient Inquiries\n",
    "In healthcare, accurate translation of medical information is critical to ensure clear communication between healthcare professionals and patients.\n",
    "\n",
    "Python Script using Hugging Face’s mBART (English to French):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a22856-2338-4d6d-b723-e5080af808ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# Load pre-trained mBART model and tokenizer\n",
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'  # mBART model for multiple languages\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Example medical inquiry in English\n",
    "medical_inquiry_in_english = \"What is the recommended treatment for high blood pressure?\"\n",
    "\n",
    "# Tokenize the text and translate to French\n",
    "tokenizer.src_lang = \"en_XX\"  # Source language: English\n",
    "tokens = tokenizer(medical_inquiry_in_english, return_tensors=\"pt\", padding=True)\n",
    "translated_tokens = model.generate(**tokens, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original (English): {medical_inquiry_in_english}\")\n",
    "print(f\"Translated (French): {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068ad2a-9ef7-4f29-9dc2-cc6999cff0c6",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The healthcare-related question \"What is the recommended treatment for high blood pressure?\" is translated from English to French using the mBART model, which is designed for multilingual translation tasks.\n",
    "This can help in translating medical information or doctor-patient communication in multilingual healthcare settings.\n",
    "Translation Techniques for Different Domains\n",
    "Banking: Translating customer inquiries such as loan status, account information, or general banking queries. MarianMT is useful for translating a wide range of banking-related terms.\n",
    "\n",
    "Insurance: Translating claim statuses, policy terms, or customer feedback to assist non-native speakers. MarianMT and mBART are both suitable for handling insurance-related data.\n",
    "\n",
    "Healthcare: Ensuring clear communication between healthcare professionals and patients by translating medical records, prescriptions, or queries. mBART provides high-quality translations across multiple languages and is effective in healthcare scenarios.\n",
    "\n",
    "Conclusion\n",
    "Translation in Natural Language Processing plays a crucial role in facilitating communication across language barriers, especially in industries like banking, insurance, and healthcare. With the advent of pre-trained models like MarianMT and mBART from Hugging Face, translation tasks have become more accessible and efficient, helping organizations serve multilingual customers and handle international data. The scripts above demonstrate how to implement translation models for various real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc36c2-d466-403b-ab8e-7f3d90221346",
   "metadata": {},
   "source": [
    "Natural Language Toolkit (NLTK) in Natural Language Processing (NLP)\n",
    "NLTK (Natural Language Toolkit) is a powerful library used for working with human language data (text). It is a comprehensive tool that provides a set of libraries and resources for various NLP tasks, including tokenization, parsing, part-of-speech tagging, stemming, lemmatization, and much more. NLTK is widely used for prototyping and research in NLP and supports multiple language processing tasks, including text processing and linguistic data analysis.\n",
    "\n",
    "Key Terminologies in NLTK\n",
    "Tokenization\n",
    "Tokenization refers to the process of splitting text into smaller units, usually words or sentences. These units are known as tokens.\n",
    "\n",
    "Word Tokenization: Splitting text into words.\n",
    "Sentence Tokenization: Splitting text into sentences.\n",
    "Stopwords\n",
    "Stopwords are common words that are typically removed from text during preprocessing, as they don't contribute much to the meaning of the sentence (e.g., \"and\", \"the\", \"is\").\n",
    "\n",
    "Stemming\n",
    "Stemming is the process of reducing a word to its base or root form. For example, \"running\" becomes \"run\", \"better\" becomes \"good\".\n",
    "\n",
    "Lemmatization\n",
    "Lemmatization is a more sophisticated version of stemming. It reduces a word to its dictionary form, considering the word’s meaning. For example, \"running\" becomes \"run\", but \"better\" becomes \"good\".\n",
    "\n",
    "Part-of-Speech (POS) Tagging\n",
    "POS tagging assigns a part-of-speech label (e.g., noun, verb, adjective) to each word in a sentence based on its context and usage.\n",
    "\n",
    "Named Entity Recognition (NER)\n",
    "NER identifies named entities such as person names, organization names, locations, dates, etc., from the text.\n",
    "\n",
    "Parsing\n",
    "Parsing is the process of analyzing the grammatical structure of a sentence and constructing a syntax tree.\n",
    "\n",
    "WordNet\n",
    "WordNet is a lexical database of English, providing synonyms, antonyms, definitions, and relationships between words. It is widely used for word sense disambiguation and understanding word meanings.\n",
    "\n",
    "Collocations\n",
    "Collocations refer to pairs or groups of words that frequently occur together (e.g., \"strong tea\", \"fast food\").\n",
    "\n",
    "NLP Tasks with NLTK in Banking, Insurance, and Healthcare Domains\n",
    "Banking: Use NLTK for customer query analysis, extracting useful information, and performing sentiment analysis on reviews or feedback.\n",
    "Insurance: Use NLTK to process insurance claims, categorize customer feedback, or extract entities like claim numbers and policy details.\n",
    "Healthcare: Use NLTK for processing medical records, patient feedback, or extracting medical entities such as disease names, treatments, and medications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cabf16-5b1e-4293-9e01-087989967a37",
   "metadata": {},
   "source": [
    "1. Banking: Analyzing Customer Queries and Extracting Information\n",
    "In a banking scenario, we can process customer queries, identify key terms like \"loan\", \"balance\", and perform sentiment analysis.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The script processes a customer query by tokenizing the sentence and words.\n",
    "It removes stopwords and applies stemming.\n",
    "It tags parts of speech and performs Named Entity Recognition (NER) to extract entities like “XYZ bank” as a named organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e61c2-91d2-4587-bcbc-292d7efa280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample customer query\n",
    "customer_query = \"What is the status of my loan application at XYZ bank?\"\n",
    "\n",
    "# Step 1: Sentence Tokenization\n",
    "sentences = sent_tokenize(customer_query)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "\n",
    "# Step 2: Word Tokenization\n",
    "words = word_tokenize(customer_query)\n",
    "print(\"Word Tokenization:\", words)\n",
    "\n",
    "# Step 3: Remove Stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words (Stopwords Removed):\", filtered_words)\n",
    "\n",
    "# Step 4: Stemming (Reducing words to root form)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Step 5: Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Step 6: Named Entity Recognition (NER)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "entities = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3327a-b9ae-4c1d-8d25-f4420f0268d2",
   "metadata": {},
   "source": [
    "2. Insurance: Processing Claims and Extracting Key Information\n",
    "For insurance, we can extract relevant details such as claim numbers, policy types, and policyholder names from claims.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The script extracts key pieces of information from an insurance claim, such as the claimant's name (\"John Doe\"), policy number (\"12345\"), and claim date (\"2023-01-15\").\n",
    "NER identifies these as entities and can be used for further processing in an automated claims system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128ef12-a2ae-4e60-ad5e-c9b005b1a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample insurance claim text\n",
    "insurance_claim = \"John Doe, policy number 12345, filed a claim for vehicle damage on 2023-01-15.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "words = word_tokenize(insurance_claim)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Step 2: Remove Stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words (Stopwords Removed):\", filtered_words)\n",
    "\n",
    "# Step 3: Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Step 4: Named Entity Recognition (NER)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "entities = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef0495-dddb-4f71-91f6-5026e88a6017",
   "metadata": {},
   "source": [
    "Healthcare: Analyzing Patient Feedback and Extracting Medical Terms\n",
    "In healthcare, extracting key medical terms and treatments can help analyze patient feedback and clinical notes.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "This script processes a patient’s feedback to identify medical-related terms such as “hypertension” (disease) and “medication” (treatment).\n",
    "By using POS tagging and NER, the system can extract important medical entities and analyze them for further use in patient record systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd4e2e-3dc1-4b0a-8176-d43d86d5a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample patient feedback text\n",
    "patient_feedback = \"I am feeling much better after taking the prescribed medication for hypertension.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "words = word_tokenize(patient_feedback)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Step 2: Remove Stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words (Stopwords Removed):\", filtered_words)\n",
    "\n",
    "# Step 3: Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Step 4: Named Entity Recognition (NER)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "entities = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eb1c6-a9f9-4648-ab2b-384f4255c9dd",
   "metadata": {},
   "source": [
    "Additional NLTK Features for Text Analysis\n",
    "WordNet: WordNet is a lexical database that provides synonyms, antonyms, and word relationships. It can be used for tasks like word sense disambiguation and finding semantic similarity between words.\n",
    "\n",
    "Example: Using WordNet for Synonym Lookup:\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The script finds synonyms for the word “loan” and prints their definitions, which can be helpful for understanding different terms used in the banking or insurance domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b14334-1926-4530-ace7-d6acaa4aa42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Finding synonyms for the word \"loan\"\n",
    "synonyms = wordnet.synsets('loan')\n",
    "for syn in synonyms:\n",
    "    print(syn.name(), syn.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c819c8-6e4b-41c1-a32a-3dca67919b2a",
   "metadata": {},
   "source": [
    "Collocations: In NLTK, collocations refer to pairs of words that frequently appear together (e.g., \"credit card\", \"insurance policy\").\n",
    "\n",
    "Example: Identifying Collocations in Text:\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The script finds common bigrams (two-word collocations) in a sample text, which is useful for identifying frequently occurring pairs like \"insurance company\" or \"policyholder claim\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3d70f-4567-4408-b870-c43d7a531af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"The policyholder's claim was approved by the insurance company.\"\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Finding bigrams (collocations)\n",
    "bigram_finder = BigramCollocationFinder.from_words(filtered_words)\n",
    "bigrams = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
    "print(\"Collocations (Bigrams):\", bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5a9a9-379d-45f8-8381-306ab14585f6",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "NLTK (Natural Language Toolkit) is a comprehensive library for text processing and NLP in Python. It is widely used for tasks like tokenization, stopword removal, stemming, lemmatization, POS tagging, NER, and more. In real-world scenarios, NLTK can be applied across various domains like Banking, Insurance, and Healthcare to automate processes such as customer query handling, claim processing, and medical record analysis.\n",
    "\n",
    "The provided Python scripts demonstrate how NLTK can be applied in these domains to perform essential text processing tasks, from extracting key entities to analyzing sentiment and identifying collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b529a-4e87-46b2-bf81-a9236bf07e03",
   "metadata": {},
   "source": [
    "SpaCy in Natural Language Processing (NLP)\n",
    "SpaCy is an open-source software library for advanced NLP in Python. It is designed for fast, efficient, and easy-to-use text processing, making it ideal for production environments and real-world applications. Unlike other NLP libraries that focus primarily on research, SpaCy is specifically engineered to be fast and reliable for large-scale industrial applications.\n",
    "\n",
    "Key Terminologies in SpaCy\n",
    "Tokenization\n",
    "Tokenization is the process of breaking text into smaller units (tokens), such as words, punctuation marks, or subwords. These tokens are the basic building blocks for further NLP tasks.\n",
    "\n",
    "Part-of-Speech (POS) Tagging\n",
    "POS tagging assigns grammatical labels (such as noun, verb, adjective, etc.) to each token based on its context within the sentence.\n",
    "\n",
    "Named Entity Recognition (NER)\n",
    "NER identifies and classifies named entities (e.g., names of people, organizations, dates, locations, etc.) within a text.\n",
    "\n",
    "Dependency Parsing\n",
    "Dependency parsing analyzes the syntactic structure of a sentence and establishes relationships between words, identifying the subject, object, and other grammatical elements.\n",
    "\n",
    "Lemmatization\n",
    "Lemmatization reduces a word to its root form or lemma. Unlike stemming, lemmatization considers the word's meaning and context to return the correct dictionary form (e.g., \"running\" becomes \"run\").\n",
    "\n",
    "Vectorization\n",
    "SpaCy also provides word vectors (e.g., word embeddings), which map words to multi-dimensional continuous vectors representing their meaning. These vectors help in capturing semantic relationships between words.\n",
    "\n",
    "Text Classification\n",
    "SpaCy allows the classification of text into predefined categories, useful for tasks like spam detection or sentiment analysis.\n",
    "\n",
    "Word Vectors and Similarity\n",
    "SpaCy uses pre-trained word vectors for determining semantic similarity between words. Words with similar meanings will have similar vectors, making it easier to measure similarity in a variety of tasks.\n",
    "\n",
    "SpaCy’s Key Features\n",
    "Pre-trained Models\n",
    "SpaCy offers multiple pre-trained models for different languages (e.g., English, German, Spanish) for tasks like POS tagging, NER, and text classification.\n",
    "\n",
    "Efficient Pipeline\n",
    "SpaCy’s pipeline allows you to process text in a highly efficient manner, enabling easy integration into production systems. It handles multiple NLP tasks sequentially (tokenization, POS tagging, NER, etc.).\n",
    "\n",
    "Integration with Deep Learning Libraries\n",
    "SpaCy can be combined with libraries like TensorFlow and PyTorch for more advanced machine learning and deep learning models.\n",
    "\n",
    "Domain-Specific Scenarios\n",
    "Banking: SpaCy can be used for customer service inquiries, extracting relevant banking terms (e.g., account types, loan status, transactions) and performing sentiment analysis on customer feedback.\n",
    "Insurance: SpaCy helps in claim processing, policy analysis, and fraud detection by extracting key entities like claim numbers, policy details, and identifying relationships between entities in claims.\n",
    "Healthcare: SpaCy can be used to extract medical terms, symptoms, treatments, and other relevant information from medical texts, such as patient records, clinical notes, and doctor-patient conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132218ca-ef8e-413d-b7f3-edf97550f3d9",
   "metadata": {},
   "source": [
    "1. Banking: Analyzing Customer Queries\n",
    "In banking, we can use SpaCy to analyze customer queries, extract important banking-related entities like “loan,” “account number,” or “balance,” and perform sentiment analysis.\n",
    "Explanation:\n",
    "\n",
    "Tokenization: Breaks the text into smaller units like words or punctuation.\n",
    "POS Tagging: Labels the parts of speech for each token (e.g., “loan” is a noun).\n",
    "NER: Identifies named entities like “XYZ Bank” (organization) and “loan” (financial product).\n",
    "Lemmatization: Converts words to their base form (e.g., “want” remains “want” because it’s already in its lemma form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88797409-51cc-42a0-bcbf-35c3a1492616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained SpaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample banking customer query\n",
    "customer_query = \"I want to know the status of my loan application at XYZ Bank.\"\n",
    "\n",
    "# Process the text through the SpaCy pipeline\n",
    "doc = nlp(customer_query)\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: POS Tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Step 3: Named Entity Recognition (NER)\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    "# Step 4: Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmas:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f37f4e-b92f-4c77-a2bd-6178fa7668df",
   "metadata": {},
   "source": [
    "2. Insurance: Claim Processing and Extracting Key Information\n",
    "In the insurance domain, we can extract key entities such as claim numbers, policy types, and customer names from a claim document.\n",
    "Explanation:\n",
    "\n",
    "NER identifies named entities such as “John Doe” (person), “12345” (policy number), and “2023-01-15” (date).\n",
    "We also manually filter tokens based on POS tags to extract relevant claim-related entities like names and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa5d9-653b-4042-9878-30071eb2b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained SpaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample insurance claim text\n",
    "insurance_claim = \"John Doe, policy number 12345, filed a claim for vehicle damage on 2023-01-15.\"\n",
    "\n",
    "# Process the text through the SpaCy pipeline\n",
    "doc = nlp(insurance_claim)\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: Named Entity Recognition (NER) to extract claim information\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    "# Step 3: Extract claim-related entities manually (using POS tags)\n",
    "claim_entities = []\n",
    "for token in doc:\n",
    "    if token.pos_ in ['PROPN', 'NUM']:\n",
    "        claim_entities.append(token.text)\n",
    "\n",
    "print(\"Claim-related Entities:\", claim_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28667ad9-44cc-4638-bee3-a4b346405034",
   "metadata": {},
   "source": [
    "3. Healthcare: Extracting Medical Terms from Patient Feedback\n",
    "In healthcare, we can use SpaCy to process patient feedback, extract key medical terms like diseases, treatments, and medications.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "NER identifies medical-related entities like “hypertension” (disease) and “medication” (treatment).\n",
    "POS tagging helps in recognizing words related to medical concepts, and lemmatization is used to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfab69-4821-427c-bdf3-5aea9bed76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained SpaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample healthcare patient feedback\n",
    "patient_feedback = \"I feel much better after taking the prescribed medication for hypertension.\"\n",
    "\n",
    "# Process the text through the SpaCy pipeline\n",
    "doc = nlp(patient_feedback)\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: POS Tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Step 3: Named Entity Recognition (NER) to extract medical entities\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    "# Step 4: Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmas:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd995f5-b432-4e15-818f-befc97af645b",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "SpaCy is a powerful, efficient, and easy-to-use library for NLP, suitable for industrial applications. It provides functionalities for tokenization, POS tagging, NER, dependency parsing, and lemmatization. In domains like Banking, Insurance, and Healthcare, SpaCy can be applied to analyze customer queries, process insurance claims, and extract medical information, making it a valuable tool for automating workflows and improving text understanding.\n",
    "\n",
    "The provided Python scripts demonstrate how SpaCy can be used to perform NLP tasks in each of these domains, with clear examples of tokenization, NER, and other key NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dac5f1-ba82-4d58-a3c7-298f76320da5",
   "metadata": {},
   "source": [
    "Computer Vision is a subfield of Artificial Intelligence (AI) that deals with enabling machines to interpret, understand, and process visual data (images and videos). Computer vision aims to simulate human vision capabilities and understand the world through visual input. It involves several key terminologies, algorithms, and techniques that help in analyzing and extracting meaningful information from images or videos.\n",
    "\n",
    "Below is a detailed explanation of the various terms and techniques in computer vision, along with Python scripts related to Banking, Insurance, and Healthcare business scenarios.\n",
    "\n",
    "Key Terminologies in Computer Vision\n",
    "Image Processing: The manipulation of images to enhance or extract features. Common tasks include filtering, resizing, and color transformations.\n",
    "\n",
    "Feature Extraction: The process of identifying important features (edges, corners, textures) in images to assist in further analysis.\n",
    "\n",
    "Object Detection: Identifying and locating objects in images. Algorithms like YOLO (You Only Look Once) or Faster R-CNN are used.\n",
    "\n",
    "Image Segmentation: Dividing an image into segments to simplify its analysis. Each segment may represent distinct objects or regions of interest.\n",
    "\n",
    "Classification: Categorizing images based on their content. A common approach is using Convolutional Neural Networks (CNNs).\n",
    "\n",
    "Tracking: Continuously locating and following the position of an object or person across multiple frames in a video.\n",
    "\n",
    "Optical Character Recognition (OCR): Recognizing and extracting text from images.\n",
    "\n",
    "Pose Estimation: Identifying and estimating the position and orientation of an object or person in an image.\n",
    "\n",
    "3D Reconstruction: Rebuilding a 3D model of a scene or object from 2D images.\n",
    "\n",
    "Deep Learning: A subset of machine learning that uses deep neural networks for tasks like classification, detection, and segmentation.\n",
    "\n",
    "Python Libraries for Computer Vision\n",
    "OpenCV: A library used for real-time computer vision.\n",
    "TensorFlow/PyTorch: Frameworks for deep learning-based computer vision tasks.\n",
    "Pillow: A Python Imaging Library (PIL) fork for simple image processing.\n",
    "Tesseract: An open-source OCR engine.\n",
    "Use Case 1: Banking - Document Verification Using OCR\n",
    "In banking, OCR can be used to verify identity documents like passports, bank statements, and cheques.\n",
    "Explanation:\n",
    "\n",
    "The cv2.imread function loads the image.\n",
    "cv2.cvtColor converts the image to grayscale, which helps OCR algorithms work better.\n",
    "pytesseract.image_to_string extracts the text from the image.\n",
    "Business Scenario:\n",
    "Use Case: A bank can use this technology to automate document verification for loan applications, ensuring that the data from the applicant’s bank statements is extracted correctly and stored for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4534274-1647-4ef6-bee7-08d34d938f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Load the image containing the bank statement\n",
    "image = cv2.imread('bank_statement.png')\n",
    "\n",
    "# Convert the image to grayscale for better OCR performance\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Use Tesseract to extract text from the image\n",
    "text = pytesseract.image_to_string(gray_image)\n",
    "\n",
    "# Display the extracted text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30b0bf-b04d-490b-9095-6a80a3935c5d",
   "metadata": {},
   "source": [
    "Use Case 2: Insurance - Claim Verification Using Object Detection\n",
    "In insurance, object detection can be used to identify and assess damage to properties (e.g., buildings, cars) for claims processing.\n",
    "Explanation:\n",
    "\n",
    "The code loads a pre-trained YOLO model and uses it to detect objects (e.g., damaged car parts).\n",
    "The cv2.dnn.readNet function loads the YOLO network with pre-trained weights.\n",
    "The bounding boxes are drawn around detected objects in the image, which could represent damages.\n",
    "Business Scenario:\n",
    "Use Case: Insurance companies can use object detection for automated claim verification. For instance, an insurance claim for a car accident can be assessed by detecting the damage in the images uploaded by the policyholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c883230-8b4a-4795-acae-c3cface9cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained YOLO model for object detection\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load an image of a damaged car\n",
    "image = cv2.imread('damaged_car.jpg')\n",
    "height, width, channels = image.shape\n",
    "\n",
    "# Prepare the image for YOLO\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Process the output and draw bounding boxes\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            cv2.rectangle(image, (center_x, center_y), (center_x + w, center_y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Show the image with bounding boxes\n",
    "cv2.imshow('Detected Objects', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29358581-9e72-4a60-a0a0-409d04ae15f3",
   "metadata": {},
   "source": [
    "Use Case 3: Healthcare - Medical Imaging and Tumor Detection\n",
    "In healthcare, computer vision can assist in detecting anomalies in medical images, such as tumors in X-rays or MRI scans.\n",
    "Explanation:\n",
    "\n",
    "This code uses a pre-trained CNN model to classify X-ray images.\n",
    "The image.load_img function loads the image, and image.img_to_array converts it to a format suitable for prediction.\n",
    "The model predicts whether the image contains a tumor or not.\n",
    "Business Scenario:\n",
    "Use Case: Healthcare providers can use this technology to assist doctors in diagnosing tumors in medical images, such as X-rays or MRIs, and streamline the diagnostic process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a50b6-f807-4104-bb7d-9e65bf3c58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a pre-trained model for tumor detection\n",
    "model = load_model('tumor_detection_model.h5')\n",
    "\n",
    "# Load and preprocess the medical image\n",
    "img = image.load_img('xray_sample.jpg', target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize the image\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display the result\n",
    "if predicted_class == 0:\n",
    "    print(\"No Tumor Detected.\")\n",
    "else:\n",
    "    print(\"Tumor Detected.\")\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(img)\n",
    "plt.title('X-ray Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863ecf4-35e0-4eac-ae2c-e9bc1e6fb1ba",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "These examples show how computer vision techniques can be applied to different business scenarios, such as banking (OCR for document verification), insurance (object detection for damage assessment), and healthcare (medical imaging for tumor detection). Python libraries like OpenCV, TensorFlow, and PyTorch provide powerful tools for implementing computer vision tasks, and pre-trained models can accelerate development in real-world applications.\n",
    "\n",
    "Each of these scenarios demonstrates how computer vision can automate complex processes and provide efficiency and accuracy in various industries.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db99a81-0d8f-45a4-ba20-c8def7cca262",
   "metadata": {},
   "source": [
    "Low-code and no-code tools are platforms that allow users to create applications with minimal or no coding experience. These tools provide drag-and-drop interfaces, pre-built templates, and integration capabilities to streamline the development process. Below is a list of popular low-code and no-code platforms, categorized based on use cases such as app development, workflow automation, database management, and business process automation.\n",
    "\n",
    "Low-Code Platforms\n",
    "OutSystems\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Full-stack application development\n",
    "Features: Supports mobile, web, and enterprise-grade applications, integrates with existing systems, provides advanced customization with code.\n",
    "Website: OutSystems\n",
    "Mendix\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Enterprise-level app development\n",
    "Features: Cloud-based platform for building mobile and web apps, enables teams to collaborate and deploy applications.\n",
    "Website: Mendix\n",
    "Appian\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Process automation and case management applications\n",
    "Features: Business process management (BPM), case management, robotic process automation (RPA), and AI-powered solutions.\n",
    "Website: Appian\n",
    "Zoho Creator\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Custom business applications\n",
    "Features: Drag-and-drop interface, workflow automation, and integration with Zoho suite and third-party apps.\n",
    "Website: Zoho Creator\n",
    "Microsoft Power Apps\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Business apps, data analytics, and automation\n",
    "Features: Connects to various data sources, integrates with Microsoft products, enables app building for mobile, web, and desktop.\n",
    "Website: Microsoft Power Apps\n",
    "Betty Blocks\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Business applications, digital transformation\n",
    "Features: Drag-and-drop builder, cloud-native, integrates with third-party systems and data sources.\n",
    "Website: Betty Blocks\n",
    "Salesforce Lightning\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: CRM-based app development\n",
    "Features: Build custom applications that integrate with Salesforce’s CRM system, drag-and-drop interface.\n",
    "Website: Salesforce Lightning\n",
    "Quick Base\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Workflow automation and custom apps\n",
    "Features: Focuses on task automation, creating business applications, and collaboration.\n",
    "Website: Quick Base\n",
    "Kissflow\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Workflow and business process automation\n",
    "Features: Workflow management, process automation, app-building tools with drag-and-drop simplicity.\n",
    "Website: Kissflow\n",
    "Pega Systems\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: CRM, business process management, and case management applications\n",
    "Features: Automates processes and customer journeys, integrates AI and RPA for intelligent automation.\n",
    "Website: Pega Systems\n",
    "No-Code Platforms\n",
    "Bubble\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Web and mobile applications\n",
    "Features: Drag-and-drop interface for creating responsive websites, apps, and database-driven applications without coding.\n",
    "Website: Bubble\n",
    "Adalo\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Mobile and web app development\n",
    "Features: Build native mobile apps and web apps using a visual editor, supports database management, and integrates with APIs.\n",
    "Website: Adalo\n",
    "Webflow\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Website design and development\n",
    "Features: Visual web design platform for responsive websites with CMS functionality and animations, integrates with third-party apps.\n",
    "Website: Webflow\n",
    "Airtable\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Database management and collaboration\n",
    "Features: Visual interface for managing databases, spreadsheets, and project management workflows, with automation and API integration.\n",
    "Website: Airtable\n",
    "Zapier\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Workflow automation and app integrations\n",
    "Features: Automates repetitive tasks by connecting over 2,000 apps and creating \"Zaps\" to trigger actions without coding.\n",
    "Website: Zapier\n",
    "Integromat (Make)\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Integration and automation of workflows\n",
    "Features: Connects apps and automates workflows, supports complex logic with no coding required.\n",
    "Website: Make\n",
    "Glide\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Mobile app creation from Google Sheets\n",
    "Features: Turns Google Sheets into mobile apps using a no-code interface.\n",
    "Website: Glide\n",
    "Thunkable\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Mobile app development\n",
    "Features: Drag-and-drop builder to create cross-platform mobile apps without writing code.\n",
    "Website: Thunkable\n",
    "Softr\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Web app development and marketplace creation\n",
    "Features: Turn Airtable data into fully functional web applications, marketplaces, and dashboards.\n",
    "Website: Softr\n",
    "Typeform\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Survey, form, and quiz creation\n",
    "Features: Build forms, surveys, and quizzes with an intuitive, drag-and-drop interface.\n",
    "Website: Typeform\n",
    "Unqork\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Enterprise-level application development\n",
    "Features: Build complex workflows and applications with no coding required. Mainly used for financial services, insurance, and government.\n",
    "Website: Unqork\n",
    "Voiceflow\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Voice app development\n",
    "Features: Build conversational interfaces and voice apps for Alexa, Google Assistant, and other platforms.\n",
    "Website: Voiceflow\n",
    "Specialized No-Code Tools\n",
    "Tilda\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Website and landing page creation\n",
    "Features: Focuses on beautiful, minimalistic design with easy-to-use blocks.\n",
    "Website: Tilda\n",
    "Carrd\n",
    "\n",
    "Type: No-code\n",
    "Use Case: Simple one-page websites\n",
    "Features: Build landing pages, personal websites, portfolios, and more.\n",
    "Website: Carrd\n",
    "Retool\n",
    "\n",
    "Type: Low-code\n",
    "Use Case: Internal tools and admin panels\n",
    "Features: Drag-and-drop builder for creating internal tools with integration to various data sources and APIs.\n",
    "Website: Retool\n",
    "Conclusion\n",
    "These low-code and no-code tools empower businesses and individuals to quickly develop applications, automate workflows, and create integrations with minimal or no programming knowledge. The choice of tool depends on the specific needs of the user, such as whether the focus is on app development, process automation, data management, or integration. For more complex and enterprise-level applications, low-code platforms like OutSystems and Mendix may be more suitable, while no-code platforms like Bubble, Glide, and Airtable offer quick and accessible solutions for smaller-scale projects and non-technical users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93452281-1dba-48ef-b54f-64c5ae4690cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c76132-c3b5-4eea-8c6d-e804528aa356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ce23d-382d-42f9-8cc3-e7ce41ec1f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048deae-2188-4468-ae1f-06ad2ad109dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754af4bc-9ebb-4e02-aa9c-ea34c8ff2480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5266c5-16f1-423d-87ea-cd772f3f1073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d32ee0-196e-43c1-bf72-f570cf65bb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51ba37-a745-4bd8-98ef-50c9f318a04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
